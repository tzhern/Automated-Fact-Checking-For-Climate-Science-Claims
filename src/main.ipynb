{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Fact Checking For Climate Science Claims\n",
    "\n",
    "Author: Zhi Hern Tom\n",
    "\n",
    "The script is capable of running on Colab using the basic T4 GPU. If running out of CUDA memory, please restart the kernel and run all. Previous progress are saved for fast recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51596e",
   "metadata": {},
   "source": [
    "In this notebook, we aim to develop and test an automated system for fact-checking claims related to climate science. The approach leverages Natural Language Processing (NLP) techniques and pretrained models to verify the authenticity of claims."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install and import requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4f81e",
   "metadata": {},
   "source": [
    "Firstly, let's set up our environment by installing the necessary libraries and packages. This ensures that we have all the tools needed to execute the subsequent code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch torchvision transformers \n",
    "!pip install pandas numpy sklearn nltk\n",
    "!pip install ipywidgets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing essential libraries for data processing, modeling, and visualization.\n",
    "\n",
    "%%capture\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Python libraries and modules for data processing, NLP tasks, and modeling.\n",
    "\n",
    "# If running on Colab, uncomment the following lines\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, if running on Google Colab, this cell mounts the Google Drive for accessing datasets or saving models.\n",
    "\n",
    "# Global variables\n",
    "\n",
    "data_dir = \"data/\"\n",
    "# Change if on Colab\n",
    "# data_dir = \"drive/MyDrive/data/\"\n",
    "\n",
    "outputs_path = \"outputs/\"\n",
    "prediction_path = \"prediction/\"\n",
    "\n",
    "train_path = f\"{data_dir}train-claims.json\"\n",
    "dev_path = f\"{data_dir}dev-claims.json\"\n",
    "evidence_path = f\"{data_dir}evidence.json\"\n",
    "\n",
    "token_len = 256\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "gpu = 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: data/\n",
      "Directory already exists: outputs/\n",
      "Directory already exists: prediction/\n"
     ]
    }
   ],
   "source": [
    "# Define utility functions to check the existence of files and directories and create them if necessary.\n",
    "\n",
    "# Check if file or directory exists\n",
    "def check_file(path):\n",
    "    if os.path.exists(path):\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"{path} does not exist.\")\n",
    "        return False\n",
    "\n",
    "def check_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {path}\")\n",
    "\n",
    "check_dir(data_dir)\n",
    "check_dir(outputs_path)\n",
    "check_dir(prediction_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section I: Evidence Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load claims, related evidences and labels\n",
    "# If label is False, return only claims and claim ids\n",
    "def load_data(path, label=False):\n",
    "    claimid_list = []\n",
    "    claim_list = []\n",
    "    if label:\n",
    "        evidences_list = []\n",
    "        label_list = []\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data:\n",
    "        claimid_list.append(item)\n",
    "        claim_list.append(data[item]['claim_text'])\n",
    "        if label:\n",
    "            evidences_list.append(data[item]['evidences'])\n",
    "            label_list.append(data[item]['claim_label'])\n",
    "    \n",
    "    if label:\n",
    "        return claimid_list, claim_list, evidences_list, label_list\n",
    "    return claimid_list, claim_list\n",
    "\n",
    "# Load evidences\n",
    "def load_evidence(path):\n",
    "    evidence_list = []\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data:\n",
    "        evidence_list.append(data[item])\n",
    "    return evidence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4e7ae",
   "metadata": {},
   "source": [
    "We need to load our claims and evidences to process them further. The following functions facilitate loading the data from JSON files, which includes claims, related evidences, and labels. The data is structured such that each claim has an associated ID, text, potential evidences, and a label indicating its veracity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load claims, related evidences, and labels from JSON files.\n",
    "\n",
    "train_claim_ids, train_claims, train_evidences, train_labels = load_data(train_path, label=True)\n",
    "dev_claim_ids, dev_claims, dev_evidences, dev_labels = load_data(dev_path, label=True)\n",
    "evidence_src = load_evidence(evidence_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc785c",
   "metadata": {},
   "source": [
    "Text data often contains noise in the form of irrelevant characters, different cases, or frequent words that don't add much semantic value. Cleaning the text involves multiple steps including removing special characters, converting to lowercase, tokenizing, removing stopwords, and lemmatizing to get to the base form of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for text cleaning, including tokenization, stopword removal, and lemmatization.\n",
    "\n",
    "# Import necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Define a set of English stopwords and initialize the WordNet Lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Define a regex pattern to clean text - keeps only alphanumeric characters and spaces\n",
    "clean_pattern = r'[^a-zA-Z0-9\\s]+'\n",
    "\n",
    "def text_clean(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the input text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The raw text to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "    - clean_text (str): The cleaned text with stopwords removed and lemmatized.\n",
    "    \"\"\"\n",
    "    # Remove special characters using the defined regex pattern\n",
    "    clean_text = re.sub(clean_pattern, '', text)\n",
    "    \n",
    "    # Tokenize the cleaned text and convert to lowercase\n",
    "    words = nltk.word_tokenize(clean_text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the words to get the final cleaned text\n",
    "    clean_text = ' '.join(words)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def evidence_clean(texts):\n",
    "    \"\"\"\n",
    "    Cleans a list of texts and saves the cleaned texts to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "    - texts (list of str): The list of raw texts to be cleaned.\n",
    "    \n",
    "    Outputs:\n",
    "    - A JSON file containing cleaned texts.\n",
    "    \"\"\"\n",
    "    clean_texts = []\n",
    "    num_texts = len(texts)\n",
    "\n",
    "    # Display a progress bar while cleaning texts\n",
    "    print(\"Cleaning texts ...\")\n",
    "    pbar = tqdm(total=num_texts, dynamic_ncols=True, miniters=10000)\n",
    "    \n",
    "    for text in texts:\n",
    "        clean_text = text_clean(text)\n",
    "        clean_texts.append(clean_text)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save cleaned texts to a JSON file\n",
    "    with open(outputs_path + 'clean_evidence.json', 'w') as f:\n",
    "        json.dump(clean_texts, f)\n",
    "    \n",
    "    print(f\"Saved to {outputs_path}clean_evidence.json\")\n",
    "    # return clean_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from outputs/clean_evidence.json\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned evidences\n",
    "if not check_file(f\"{outputs_path}clean_evidence.json\"):\n",
    "    evidence_clean(evidence_src) \n",
    "with open(outputs_path + 'clean_evidence.json', 'r') as f:\n",
    "    clean_evidence_src = json.load(f)\n",
    "    print(f\"Loaded from {outputs_path}clean_evidence.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c5a30",
   "metadata": {},
   "source": [
    "Jaccard similarity is a measure of similarity between two sets. It's defined as the size of the intersection divided by the size of the union of the two sets. For our purpose, we'll treat each text as a set of words and compute the Jaccard similarity to filter relevant evidence for a given claim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad3154",
   "metadata": {},
   "source": [
    "# Define functions to compute Jaccard similarity between two texts and use it to filter the top-k most similar evidences for a given claim.\n",
    "\n",
    "Using the text cleaning functions, we can process our evidence data. This step ensures that the evidence is in a format suitable for further analysis. The cleaned evidence is then saved to a JSON file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned evidences from the saved JSON file. If the cleaned data doesn't exist, we process the raw evidence data.\n",
    "\n",
    "# Compute jaccard similarity between two texts\n",
    "def jaccard_similarity(s1, s2):\n",
    "    set1 = set(s1.split())\n",
    "    set2 = set(s2.split())\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    similarity = len(intersection) / len(union)\n",
    "    return similarity\n",
    "\n",
    "# Use Jaccard similarity to filter evidences\n",
    "def jaccard_filter(claim, k=100):\n",
    "    clean_claim = text_clean(claim)\n",
    "    res = []\n",
    "    for i, ev in enumerate(clean_evidence_src):\n",
    "        res.append((i, jaccard_similarity(clean_claim, ev)))\n",
    "    return sorted(res, key = lambda x: x[1], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307247c7",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects the importance of a word in a document relative to a corpus. By converting our texts into TF-IDF vectors, we can compute the cosine similarity between them. This allows us to filter the evidence that is most similar to a given claim based on their TF-IDF representations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to compute TF-IDF vectors for texts, calculate cosine similarity between them, and use the similarity to filter the top-k most similar evidences for a given claim.\n",
    "\n",
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TF-IDF vectors for all cleaned evidences.\n",
    "\n",
    "# Perform TF-IDF on given texts\n",
    "def tfidf_evidence(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(texts)\n",
    "    return tfidf_vectors, tfidf_vectorizer\n",
    "\n",
    "# Cosine similarity between TF-IDF vectors of claim and evidence\n",
    "def tfidf_similarity(claim, ev_id):\n",
    "    evidence = clean_evidence_src[ev_id]\n",
    "    claim_vector = ev_tfidf_vectorizer.transform([claim])\n",
    "    evidence_vector = ev_tfidf_vectorizer.transform([evidence])\n",
    "    similarity = cosine_similarity(claim_vector, evidence_vector)\n",
    "    return similarity.item()\n",
    "\n",
    "# Use TF-IDF similarity to filter evidences\n",
    "def tfidf_filter(claim, k=100):\n",
    "    claim = text_clean(claim)\n",
    "    claim_vector = ev_tfidf_vectorizer.transform([claim])\n",
    "    similarities = cosine_similarity(claim_vector, ev_tfidf_vectors)\n",
    "    top_k_indices = np.argsort(similarities, axis=-1)[:, -k:].flatten()\n",
    "    top_k_scores = np.sort(similarities, axis=-1)[:, -k:].flatten()\n",
    "    return list(zip(top_k_indices, top_k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectors generated.\n"
     ]
    }
   ],
   "source": [
    "# Perform TF-IDF on all evidences\n",
    "ev_tfidf_vectors, ev_tfidf_vectorizer = tfidf_evidence(clean_evidence_src)\n",
    "print(\"TF-IDF vectors generated.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d7dfd",
   "metadata": {},
   "source": [
    "DistilBERT is a distilled version of BERT, a popular transformer model for NLP tasks. By using DistilBERT, we can generate embeddings for our texts, which can then be used to compute cosine similarity between them. This provides another method to filter evidence that is most similar to a given claim based on their DistilBERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to generate embeddings for texts using DistilBERT, compute cosine similarity between these embeddings, and filter the top-k most similar evidences for a given claim.\n",
    "\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Generate embeddings for evidences using DistilBERT\n",
    "def get_text_embedding(evidences, model, tokenizer, batch_size=64, show=True):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of evidences using the DistilBERT model.\n",
    "    \n",
    "    Args:\n",
    "    - evidences (list): A list of text evidence.\n",
    "    - model (transformers.Model): A DistilBERT model.\n",
    "    - tokenizer (transformers.Tokenizer): A DistilBERT tokenizer.\n",
    "    - batch_size (int): Size of each batch for processing. Default is 64.\n",
    "    - show (bool): Flag to display a progress bar. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "    - embeddings (numpy.ndarray): Embeddings for the input evidences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setting the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # List to store the generated embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Splitting the evidences into batches\n",
    "    batches = [evidences[i:i+batch_size] for i in range(0, len(evidences), batch_size)]\n",
    "    \n",
    "    # Progress bar initialization (if show is True)\n",
    "    pbar = tqdm(total=len(batches), dynamic_ncols=True, miniters=1000) if show else None\n",
    "\n",
    "    for batch in batches:\n",
    "        # Tokenizing the batched evidences and getting the necessary inputs for the model\n",
    "        tokenized = tokenizer.batch_encode_plus(batch, padding=True, truncation=True, max_length=token_len, return_tensors='pt')\n",
    "        input_ids = tokenized['input_ids']\n",
    "        attention_masks = tokenized['attention_mask']\n",
    "\n",
    "        # Moving the tokenized data to the specified device (e.g., GPU)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        \n",
    "        # Compute the embeddings for the batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_masks)\n",
    "            \n",
    "            # Extracting the [CLS] token's embeddings for each evidence, as it's a good representation of the entire text.\n",
    "            batch_embeddings = outputs[0][:, 0, :].cpu().numpy()\n",
    "        \n",
    "        # Storing the batch embeddings\n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "        # Updating the progress bar (if show is True)\n",
    "        pbar.update(1) if show else None\n",
    "    \n",
    "    # Closing the progress bar (if show is True)\n",
    "    pbar.close() if show else None\n",
    "\n",
    "    # Stacking the batched embeddings into a single numpy array\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from outputs/ev_distilbert.npy\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings for all evidences\n",
    "ev_embeddings = None\n",
    "if not check_file(f\"{outputs_path}ev_distilbert.npy\"):\n",
    "    print(\"Generating embeddings for evidences using DistilBERT ...\")\n",
    "    ev_embeddings = get_text_embedding(evidence_src, distilbert_model, distilbert_tokenizer)\n",
    "    np.save(outputs_path + \"ev_distilbert.npy\", ev_embeddings)\n",
    "else:\n",
    "    ev_embeddings = np.load(outputs_path + \"ev_distilbert.npy\")\n",
    "    print(f\"Loaded from {outputs_path}ev_distilbert.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter evidences based on cosine similarity between DistilBERT embeddings of claim and evidence\n",
    "def distilbert_filter(claim, k=100):\n",
    "    claim_embedding = get_text_embedding([claim], distilbert_model, distilbert_tokenizer, show=False)\n",
    "    similarities = cosine_similarity(claim_embedding, ev_embeddings)\n",
    "    top_k_indices = np.argsort(similarities, axis=-1)[:, -k:].flatten()\n",
    "    top_k_scores = np.sort(similarities, axis=-1)[:, -k:].flatten()\n",
    "    return list(zip(top_k_indices, top_k_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e703eb",
   "metadata": {},
   "source": [
    "To validate the performance of our evidence retrieval methods, we need an evaluation mechanism. The following functions provide a way to evaluate how well a given filter (Jaccard, TF-IDF, DistilBERT) retrieves relevant evidence for a claim. The performance is measured by checking the retrieved evidence against the ground truth."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to evaluate the performance of a given filter on individual claims and on the entire development set.\n",
    "\n",
    "#### Evaluation on filters (Jaccard, TF-IDF, DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the performance of a filter on a specific claim\n",
    "# Parameters:\n",
    "# - index: Index of the claim to evaluate\n",
    "# - filter_func: The function to use for filtering evidences for the claim\n",
    "# - k: Number of evidences to retrieve (default is 100)\n",
    "# - show: Whether to print the results for each evidence (default is True)\n",
    "def eval_filter(index, filter_func, k=100, show=True):\n",
    "    # Retrieve the claim from the dev set using the provided index\n",
    "    claim = dev_claims[index]\n",
    "    \n",
    "    # Retrieve the true evidences associated with the claim\n",
    "    truths = dev_evidences[index]\n",
    "    \n",
    "    # Apply the filtering function to the claim to get the top k evidences\n",
    "    evidences = [item[0] for item in filter_func(claim, k)]\n",
    "    \n",
    "    # Initialize counters for true and false predictions\n",
    "    t = 0\n",
    "    f = 0\n",
    "    \n",
    "    # Loop through each true evidence and check if it's in the retrieved evidences\n",
    "    for truth in truths:\n",
    "        if int(truth[9:]) in evidences:     # evidence ids are of the form 'evidence-xxx'\n",
    "            t += 1\n",
    "            print(f\"In: {truth}\") if show else None\n",
    "        else:\n",
    "            f += 1\n",
    "            print(f\"Out: {truth}\") if show else None\n",
    "    \n",
    "    # Print the total number of true and false evidence predictions\n",
    "    print(f\"In: {t}, Out: {f}\") if show else None\n",
    "    \n",
    "    # Return the counts for further processing or evaluation\n",
    "    return t, f\n",
    "\n",
    "# Function to evaluate the performance of a filter on the entire dev set\n",
    "# Parameters:\n",
    "# - filter_func: The function to use for filtering evidences for the claim\n",
    "# - k: Number of evidences to retrieve for each claim (default is 100)\n",
    "def eval_filter_dev(filter_func, k=100):\n",
    "    # Initialize counters for true positives and false negatives\n",
    "    t = 0\n",
    "    f = 0\n",
    "    \n",
    "    # Initialize a progress bar to provide feedback during evaluation\n",
    "    pbar = tqdm(total=len(dev_claims), dynamic_ncols=True)\n",
    "    \n",
    "    # Loop through each claim in the dev set and evaluate it\n",
    "    for i in range(len(dev_claims)):\n",
    "        t_, f_ = eval_filter(i, filter_func, k, False)\n",
    "        t += t_\n",
    "        f += f_\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    # Print the aggregated results for the entire dev set\n",
    "    print(f\"In: {t}, Out: {f}, Total: {t+f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4d966",
   "metadata": {},
   "source": [
    "# Functions to evaluate the performance of a given filter on individual claims and on the entire development set. Performance is assessed based on the correct retrieval of relevant evidences.\n",
    "\n",
    "To quantify the performance of our evidence retrieval methods, it's vital to evaluate them against a benchmark. Below are functions that assess the effectiveness of various filters on claims. The evaluation is based on the number of relevant evidences correctly retrieved (true positives) and those missed (false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476abae339634cbfab2ba00fcf09abb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In: 174, Out: 317, Total: 491\n"
     ]
    }
   ],
   "source": [
    "eval_filter_dev(jaccard_filter, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e082ca2e70124abebcb853ab3cea937d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In: 186, Out: 305, Total: 491\n"
     ]
    }
   ],
   "source": [
    "eval_filter_dev(tfidf_filter, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3434db97f6c54835bed503fc3f0cee75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In: 115, Out: 376, Total: 491\n"
     ]
    }
   ],
   "source": [
    "eval_filter_dev(distilbert_filter, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad978148",
   "metadata": {},
   "source": [
    "For training machine learning models, it's essential to have both positive examples (relevant evidences) and negative examples (irrelevant evidences). The following functions facilitate pairing claims with both types of evidences and labels. Negative examples can be sampled in two ways: randomly from the dataset or by selecting 'hard' negatives that are most similar to the claim based on TF-IDF."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to pair claims with evidence and labels. It includes functions for sampling negative examples in two ways: random sampling and hard negative sampling using TF-IDF similarity.\n",
    "\n",
    "#### BERT for evidence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable storing the evidences\n",
    "num_evidences = len(evidence_src) - 1\n",
    "\n",
    "# Function to randomly select n negative cases\n",
    "def get_rand_negative_ids(evidence_ids, n):\n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        temp_id = random.randint(0, num_evidences)\n",
    "        # Ensure the randomly selected ID is not already in evidence_ids or res\n",
    "        while temp_id in evidence_ids or temp_id in res:\n",
    "            temp_id = random.randint(0, num_evidences)\n",
    "        res.append(temp_id)\n",
    "    return res\n",
    "\n",
    "# Function to select n negative cases that have the highest similarity to the claim\n",
    "def get_hard_negative_ids(claim, evidence_ids, filter_func, n):\n",
    "    res = []\n",
    "    # Retrieve similar_ids by sorting through similarities returned by filter_func\n",
    "    similar_ids = filter_func(claim, 50)\n",
    "    cnt = 0\n",
    "    for i in range(n):\n",
    "        temp_id = similar_ids[cnt][0]\n",
    "        # Skip the IDs that are already used as positive evidence\n",
    "        while temp_id in evidence_ids:\n",
    "            cnt += 1\n",
    "            temp_id = similar_ids[cnt][0]\n",
    "        res.append(temp_id)\n",
    "        cnt += 1\n",
    "    return res\n",
    "\n",
    "# Function to create DataFrame consisting of claim-evidence pairs along with labels\n",
    "def pair_claim_evidence(claims, evs, hard=False, show=False):\n",
    "    claim_list = []\n",
    "    ev_list = []\n",
    "    labels = []\n",
    "    \n",
    "    # Optional progress bar\n",
    "    pbar = tqdm(total=len(claims), dynamic_ncols=True) if show else None\n",
    "    \n",
    "    # Loop through each claim\n",
    "    for i, claim in enumerate(claims):\n",
    "        # Add positive cases\n",
    "        raw_evidence_ids = evs[i]\n",
    "        evidence_ids = [int(num[9:]) for num in raw_evidence_ids]\n",
    "        for evidence_id in evidence_ids:\n",
    "            evidence_text = evidence_src[evidence_id]\n",
    "            claim_list.append(claim)\n",
    "            ev_list.append(evidence_text)\n",
    "            labels.append(1)\n",
    "        \n",
    "        # Add negative cases\n",
    "        num_positive = len(evidence_ids)\n",
    "        if hard:\n",
    "            # For hard negatives\n",
    "            negative_ids = get_hard_negative_ids(claim, evidence_ids, tfidf_filter, num_positive)\n",
    "        else:\n",
    "            # For random negatives\n",
    "            negative_ids = get_rand_negative_ids(evidence_ids, num_positive)\n",
    "        \n",
    "        for num in negative_ids:\n",
    "            evidence_text = evidence_src[num]\n",
    "            claim_list.append(claim)\n",
    "            ev_list.append(evidence_text)\n",
    "            labels.append(0)\n",
    "        \n",
    "        # Update the progress bar, if enabled\n",
    "        pbar.update(1) if show else None\n",
    "    \n",
    "    # Close the progress bar, if enabled\n",
    "    pbar.close() if show else None\n",
    "    \n",
    "    return pd.DataFrame({'Claim': claim_list, 'Evidence': ev_list, 'Label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and development sets using the pairing functions. This ensures that each set has a balanced mix of positive and negative examples.\n",
    "\n",
    "# Create training and dev sets using random sampling\n",
    "train_evcls = pair_claim_evidence(train_claims, train_evidences)\n",
    "dev_evcls = pair_claim_evidence(dev_claims, dev_evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the training and development sets to create a comprehensive dataset for training and validation.\n",
    "\n",
    "# Merge train and dev sets (random sampling)\n",
    "train_evcls_f = pd.merge(train_evcls, dev_evcls, on=['Claim', 'Evidence', 'Label'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and dev sets using hard negative sampling\n",
    "train_evcls_hard = None\n",
    "dev_evcls_hard = None\n",
    "train_evcls_hard_data = \"evcls_hard_train.csv\"\n",
    "dev_evcls_hard_data = \"evcls_hard_dev.csv\"\n",
    "\n",
    "if not check_file(f\"{outputs_path}{train_evcls_hard_data}\"):\n",
    "    train_evcls_hard = pair_claim_evidence(train_claims, train_evidences, hard=True, show=True)\n",
    "    train_evcls_hard.to_csv(f\"{outputs_path}{train_evcls_hard_data}\", index=False)\n",
    "    print(f\"Saved to {outputs_path}{train_evcls_hard_data}\")\n",
    "else:\n",
    "    train_evcls_hard = pd.read_csv(f\"{outputs_path}{train_evcls_hard_data}\")\n",
    "\n",
    "if not check_file(f\"{outputs_path}{dev_evcls_hard_data}\"):\n",
    "    dev_evcls_hard = pair_claim_evidence(dev_claims, dev_evidences, hard=True, show=True)\n",
    "    dev_evcls_hard.to_csv(f\"{outputs_path}{dev_evcls_hard_data}\", index=False)\n",
    "    print(f\"Saved to {outputs_path}{dev_evcls_hard_data}\")\n",
    "else:\n",
    "    dev_evcls_hard = pd.read_csv(f\"{outputs_path}{dev_evcls_hard_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and dev sets (hard negative sampling)\n",
    "train_evcls_hard_f = pd.merge(train_evcls_hard, dev_evcls_hard, on=['Claim', 'Evidence', 'Label'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to BERT input format\n",
    "def convert_input(claim, evidence, tokenizer, maxlen):\n",
    "    \"\"\"\n",
    "    Convert claim and evidence texts into BERT input format.\n",
    "    \n",
    "    Parameters:\n",
    "    - claim (str): The claim text.\n",
    "    - evidence (str): The evidence text supporting/refuting the claim.\n",
    "    - tokenizer: The BERT tokenizer used for tokenizing the input texts.\n",
    "    - maxlen (int): Maximum length for the tokenized sequence.\n",
    "    \n",
    "    Returns:\n",
    "    - tokens_ids_t (torch.Tensor): Token IDs tensor.\n",
    "    - attn_mask_t (torch.Tensor): Attention mask tensor.\n",
    "    - seg_ids_t (torch.Tensor): Segment IDs tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the claim and evidence.\n",
    "    claim_tokens = tokenizer.tokenize(claim)\n",
    "    evidence_tokens = tokenizer.tokenize(evidence)\n",
    "\n",
    "    # Truncate tokens to fit the maximum length.\n",
    "    # It ensures that the combined length of claim and evidence tokens fits within maxlen.\n",
    "    while len(claim_tokens) + len(evidence_tokens) > maxlen - 3:\n",
    "        if len(claim_tokens) > len(evidence_tokens):\n",
    "            claim_tokens.pop()\n",
    "        else:\n",
    "            evidence_tokens.pop()\n",
    "\n",
    "    # Arrange tokens with BERT's [CLS], [SEP] and [PAD] tokens.\n",
    "    # Format: [CLS] + evidence + [SEP] + claim + [SEP] + [PAD] (if needed).\n",
    "    tokens = ['[CLS]'] + evidence_tokens + ['[SEP]'] + claim_tokens + ['[SEP]']\n",
    "    if len(tokens) < maxlen:\n",
    "        tokens = tokens + ['[PAD]' for _ in range(maxlen - len(tokens))]\n",
    "\n",
    "    # Create attention mask - 0 for [PAD] tokens, 1 for all others.\n",
    "    attn_mask = [0 if token == '[PAD]' else 1 for token in tokens]\n",
    "\n",
    "    # Create segment IDs - 0 for evidence tokens, 1 for claim tokens.\n",
    "    # Note: It also accounts for [CLS] and the first [SEP] tokens.\n",
    "    seg_ids = [0] * (len(evidence_tokens) + 2) + [1] * (maxlen - len(evidence_tokens) - 2)\n",
    "    \n",
    "    # Convert tokens to their respective IDs.\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Convert lists to PyTorch tensors.\n",
    "    tokens_ids_t = torch.tensor(tokens_ids)\n",
    "    attn_mask_t = torch.tensor(attn_mask)\n",
    "    seg_ids_t   = torch.tensor(seg_ids)\n",
    "\n",
    "    return tokens_ids_t, attn_mask_t, seg_ids_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for evidence classification\n",
    "class EVCLSDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, datasrc, maxlen):\n",
    "        self.data = datasrc\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.maxlen = maxlen\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        claim = self.data.loc[index, 'Claim']\n",
    "        evidence = self.data.loc[index, 'Evidence']\n",
    "        label = self.data.loc[index, 'Label']\n",
    "\n",
    "        tokens_ids_t, attn_mask_t, seg_ids_t = convert_input(claim, evidence, self.tokenizer, self.maxlen)\n",
    "        return tokens_ids_t, attn_mask_t, seg_ids_t, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and dev data loader (random sampling)\n",
    "train_evcls_set = EVCLSDataset(train_evcls, token_len)\n",
    "dev_evcls_set = EVCLSDataset(dev_evcls, token_len)\n",
    "train_evcls_loader = DataLoader(train_evcls_set, batch_size = 32, shuffle=True)\n",
    "dev_evcls_loader = DataLoader(dev_evcls_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and dev data loader (hard negative sampling)\n",
    "train_evcls_hard_set = EVCLSDataset(train_evcls_hard, token_len)\n",
    "dev_evcls_hard_set = EVCLSDataset(dev_evcls_hard, token_len)\n",
    "train_evcls_hard_loader = DataLoader(train_evcls_hard_set, batch_size = 32, shuffle=True)\n",
    "dev_evcls_hard_loader = DataLoader(dev_evcls_hard_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for merged dataset (random sampling)\n",
    "train_evcls_f_set = EVCLSDataset(train_evcls_f, token_len)\n",
    "train_evcls_f_loader = DataLoader(train_evcls_f_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for merged dataset (hard negative sampling)\n",
    "train_evcls_hard_f_set = EVCLSDataset(train_evcls_hard_f, token_len)\n",
    "train_evcls_hard_f_loader = DataLoader(train_evcls_hard_f_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sturcture for model of evidence classification\n",
    "class EvidenceClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    EvidenceClassifier is a model designed to classify a given sequence \n",
    "    (typically text) based on the evidence presented within it using BERT.\n",
    "\n",
    "    Attributes:\n",
    "    - bert_layer (BertModel): Pre-trained BERT model used for extracting sequence representations.\n",
    "    - cls_layer (nn.Linear): Linear layer for classification based on BERT's [CLS] token representation.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the model components.\n",
    "        \"\"\"\n",
    "        super(EvidenceClassifier, self).__init__()\n",
    "        \n",
    "        # Load a pre-trained BERT model. This will automatically download the model\n",
    "        # the first time you run it and will use the 'bert-base-uncased' variant.\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Define a linear classifier that will be used on top of the BERT [CLS] token representation.\n",
    "        # 768 is the size of the hidden representation from 'bert-base-uncased' and \n",
    "        # we're outputting a single value as our classification result.\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg_ids):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "        - seq (torch.Tensor): Input sequence tensor.\n",
    "        - attn_masks (torch.Tensor): Attention masks to avoid attention to padding tokens.\n",
    "        - seg_ids (torch.Tensor): Segment IDs used in BERT for distinguishing different sequences.\n",
    "\n",
    "        Returns:\n",
    "        - logits (torch.Tensor): Output logits for the classifier.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain BERT's last hidden state from the transformer.\n",
    "        # The 'last_hidden_state' is a sequence of hidden states of the last layer of the model.\n",
    "        outputs = self.bert_layer(seq, attention_mask=attn_masks, token_type_ids=seg_ids, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "        \n",
    "        # Extract the [CLS] token's representations which is the first token in BERT's output.\n",
    "        # This representation is often used for classification tasks.\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        \n",
    "        # Pass the [CLS] token representation through our classifier\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy for model of evidence classification\n",
    "def acc_sigmoid(logits, labels):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for sigmoid-based binary classification.\n",
    "    \n",
    "    Args:\n",
    "    - logits (torch.Tensor): The raw model output (before activation function).\n",
    "    - labels (torch.Tensor): True labels.\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: Accuracy of the model predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply sigmoid activation function to logits to get probabilities\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    \n",
    "    # Convert probabilities to binary values (0 or 1) using threshold 0.5\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    \n",
    "    # Compute the accuracy by comparing predicted values with true labels\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# This function evaluates the model performance on a given dataset (e.g., dev set)\n",
    "def evaluate(model, criterion, devloader, acc_func, cls_type=0):\n",
    "    \"\"\"\n",
    "    Evaluate a model's performance on a dataset.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to be evaluated.\n",
    "    - criterion (torch.nn.Module): Loss function.\n",
    "    - devloader (torch.utils.data.DataLoader): DataLoader for the dataset.\n",
    "    - acc_func (function): Function to compute accuracy.\n",
    "    - cls_type (int): Type of classification. 0 for evidence classification, 1 for claim classification. Default is 0.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple[torch.Tensor, float]: Mean accuracy and mean loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to evaluation mode (deactivates dropout and batch normalization)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize mean accuracy and mean loss to 0\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    \n",
    "    # Ensure no gradients are computed to save memory and speed up computation\n",
    "    with torch.no_grad():\n",
    "        # Loop through batches of data in the dataset\n",
    "        for seq, attn_masks, seg_ids, labels in devloader:\n",
    "            \n",
    "            # Move tensors to GPU if available\n",
    "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
    "            \n",
    "            # Get model predictions (logits)\n",
    "            logits = model(seq, attn_masks, seg_ids)\n",
    "            \n",
    "            # Compute the loss using provided criterion\n",
    "            if cls_type == 0:  # For evidence classification\n",
    "                mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            else:  # For claim classification\n",
    "                mean_loss += criterion(logits, labels).item()\n",
    "            \n",
    "            # Update mean accuracy using provided accuracy function\n",
    "            mean_acc += acc_func(logits, labels)\n",
    "            count += 1\n",
    "    \n",
    "    # Return mean accuracy and mean loss\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encaplusate the training process\n",
    "# acc_func: accuracy function (sigmoid or softmax)\n",
    "# cls_type: 0 for evidence classfication, 1 for claim classification\n",
    "# full: True for using the merged dataset\n",
    "# Based on the code from week 7's workshop\n",
    "def train(model, criterion, optimizer, train_loader, dev_loader, max_eps, acc_func, name, cls_type=0, full=False):\n",
    "    \"\"\"\n",
    "    Train the given model using the provided data and hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to be trained.\n",
    "    - criterion: Loss function used for training.\n",
    "    - optimizer: Optimization algorithm.\n",
    "    - train_loader: DataLoader providing training data in batches.\n",
    "    - dev_loader: DataLoader providing development (validation) data in batches.\n",
    "    - max_eps: Maximum number of epochs for training.\n",
    "    - acc_func: Function to compute accuracy.\n",
    "    - name: Name of the saved model.\n",
    "    - cls_type: Classifier type (default is 0). Determines how to compute the loss.\n",
    "    - full: If True, training is done using a merged dataset (default is False).\n",
    "\n",
    "    Returns:\n",
    "    None. The model's state_dict is saved to a file if performance improves.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If using merged dataset, print a message\n",
    "    if full:\n",
    "        print(\"Using merged dataset ...\")\n",
    "\n",
    "    best_acc = 0  # Keep track of the best accuracy seen during training\n",
    "    st = time.time()  # Start time for tracking duration\n",
    "\n",
    "    # Iterate over the epochs\n",
    "    for ep in range(max_eps):\n",
    "        print(f\"Epoch {ep} ...\")\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        # Iterate over batches of data from the train_loader\n",
    "        for i, (seq, attn_masks, seg_ids, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()  # Clear any accumulated gradients\n",
    "            \n",
    "            # Move data to GPU for faster computation\n",
    "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
    "            \n",
    "            # Forward pass: compute predictions\n",
    "            logits = model(seq, attn_masks, seg_ids)\n",
    "            \n",
    "            # Compute the loss; consider cls_type for loss calculation\n",
    "            loss = criterion(logits.squeeze(-1), labels.float()) if cls_type == 0 else criterion(logits, labels)\n",
    "            loss.backward()  # Compute gradient of the loss with respect to model parameters\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            # Print training stats every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                acc = acc_func(logits, labels)  # Compute accuracy for current batch\n",
    "                print(f\"Iteration {i} of epoch {ep} complete. Loss: {loss.item()}; Accuracy: {acc}; Time: {round((time.time() - st), 2)}s\")\n",
    "                st = time.time()\n",
    "\n",
    "        # If not in 'full' mode, evaluate the model on the development set after each epoch\n",
    "        if not full:\n",
    "            dev_acc, dev_loss = evaluate(model, criterion, dev_loader, acc_func, cls_type=cls_type)\n",
    "            print(f\"\\nEpoch {ep} completed. Development Accuracy: {dev_acc}; Development Loss: {dev_loss}\\n\")\n",
    "            \n",
    "            # Save the model if its performance on the dev set has improved\n",
    "            if dev_acc > best_acc:\n",
    "                print(f\"Best accuracy is improved from {best_acc} to {dev_acc}\")\n",
    "                best_acc = dev_acc\n",
    "                torch.save(model.state_dict(), f\"{outputs_path}{name}.dat\")\n",
    "                print(f\"Model is saved to {outputs_path}{name}.dat\\n\")\n",
    "        else:\n",
    "            # If in 'full' mode, save the model after every epoch\n",
    "            torch.save(model.state_dict(), f\"{outputs_path}{name}.dat\")\n",
    "            print(f\"Model is saved to {outputs_path}{name}.dat\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/evcls.dat does not exist.\n",
      "Epoch 0 ...\n",
      "Iteration 0 of epoch 0 complete. Loss: 0.8964952230453491; Accuracy: 0.125; Time: 3.22s\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.011854280717670918; Accuracy: 1.0; Time: 21.02s\n",
      "Iteration 200 of epoch 0 complete. Loss: 0.006558247376233339; Accuracy: 1.0; Time: 21.0s\n",
      "\n",
      "Epoch 0 completed. Development Accuracy: 0.9879031777381897; Development Loss: 0.035833008328242405\n",
      "\n",
      "Best accuracy is improved from 0 to 0.9879031777381897\n",
      "Model is saved to outputs/evcls.dat\n",
      "\n",
      "Epoch 1 ...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.013068395666778088; Accuracy: 1.0; Time: 15.82s\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.0016046068631112576; Accuracy: 1.0; Time: 21.06s\n",
      "Iteration 200 of epoch 1 complete. Loss: 0.001259674783796072; Accuracy: 1.0; Time: 21.02s\n",
      "\n",
      "Epoch 1 completed. Development Accuracy: 0.9899193048477173; Development Loss: 0.04143776897821696\n",
      "\n",
      "Best accuracy is improved from 0.9879031777381897 to 0.9899193048477173\n",
      "Model is saved to outputs/evcls.dat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train: random sampling\n",
    "evcls_name = \"evcls\"\n",
    "\n",
    "# Check if the model file for the evidence classifier already exists.\n",
    "# If it doesn't, then proceed to initialize and train the model.\n",
    "if not check_file(f\"{outputs_path}{evcls_name}.dat\"):\n",
    "    \n",
    "    # Instantiate the EvidenceClassifier model.\n",
    "    evcls_model = EvidenceClassifier()\n",
    "    \n",
    "    # Move the model to the specified device, e.g., GPU or CPU.\n",
    "    evcls_model.to(device)\n",
    "    \n",
    "    # Define the loss function for the evidence classifier. \n",
    "    # Binary Cross Entropy with Logits Loss is suitable for binary classification tasks.\n",
    "    evcls_criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Define the optimizer for the model, using the Adam optimization algorithm.\n",
    "    # A learning rate of 2e-5 is specified.\n",
    "    evcls_optimizer = optim.Adam(evcls_model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Number of training epochs.\n",
    "    num_epoch = 2\n",
    "    \n",
    "    # Train the evidence classifier model.\n",
    "    # This function is assumed to train the model over the specified epochs, \n",
    "    # using the given loss function, optimizer, training data loader, \n",
    "    # and development data loader, and then saves the trained model.\n",
    "    train(evcls_model, evcls_criterion, evcls_optimizer, train_evcls_loader, dev_evcls_loader, num_epoch, acc_sigmoid, evcls_name)\n",
    "\n",
    "# If the model file already exists, print a message indicating the same.\n",
    "else:\n",
    "    print(f\"{outputs_path}{evcls_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/evcls_hard.dat does not exist.\n",
      "Epoch 0 ...\n",
      "Iteration 0 of epoch 0 complete. Loss: 0.7445584535598755; Accuracy: 0.375; Time: 0.22s\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.4154742956161499; Accuracy: 0.75; Time: 21.0s\n",
      "Iteration 200 of epoch 0 complete. Loss: 0.4395042061805725; Accuracy: 0.78125; Time: 21.0s\n",
      "\n",
      "Epoch 0 completed. Development Accuracy: 0.8561216592788696; Development Loss: 0.33622004860831844\n",
      "\n",
      "Best accuracy is improved from 0 to 0.8561216592788696\n",
      "Model is saved to outputs/evcls_hard.dat\n",
      "\n",
      "Epoch 1 ...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.2198200523853302; Accuracy: 0.90625; Time: 15.81s\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.16976988315582275; Accuracy: 0.96875; Time: 21.0s\n",
      "Iteration 200 of epoch 1 complete. Loss: 0.20921653509140015; Accuracy: 0.96875; Time: 20.93s\n",
      "\n",
      "Epoch 1 completed. Development Accuracy: 0.869043231010437; Development Loss: 0.33505730066568623\n",
      "\n",
      "Best accuracy is improved from 0.8561216592788696 to 0.869043231010437\n",
      "Model is saved to outputs/evcls_hard.dat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train: hard negative sampling\n",
    "evcls_hard_name = \"evcls_hard\"\n",
    "if not check_file(f\"{outputs_path}{evcls_hard_name}.dat\"):\n",
    "    evcls_hard_model = EvidenceClassifier()\n",
    "    evcls_hard_model.to(device)\n",
    "    evcls_hard_criterion = nn.BCEWithLogitsLoss()\n",
    "    evcls_hard_optimizer = optim.Adam(evcls_hard_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 2\n",
    "    train(evcls_hard_model, evcls_hard_criterion, evcls_hard_optimizer, train_evcls_hard_loader, dev_evcls_hard_loader, num_epoch, acc_sigmoid, evcls_hard_name)\n",
    "else:\n",
    "    print(f\"{outputs_path}{evcls_hard_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/evcls_f.dat does not exist.\n",
      "Using merged dataset ...\n",
      "Epoch 0 ...\n",
      "Iteration 0 of epoch 0 complete. Loss: 0.3497883677482605; Accuracy: 1.0; Time: 0.21s\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.03620237484574318; Accuracy: 0.96875; Time: 21.03s\n",
      "Iteration 200 of epoch 0 complete. Loss: 0.011501314118504524; Accuracy: 1.0; Time: 21.06s\n",
      "Model is saved to outputs/evcls_f.dat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train: random sampling (merged)\n",
    "evcls_f_name = \"evcls_f\"\n",
    "if not check_file(f\"{outputs_path}{evcls_f_name}.dat\"):\n",
    "    evcls_f_model = EvidenceClassifier()\n",
    "    evcls_f_model.to(device)\n",
    "    evcls_f_criterion = nn.BCEWithLogitsLoss()\n",
    "    evcls_f_optimizer = optim.Adam(evcls_f_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 1\n",
    "    train(evcls_f_model, evcls_f_criterion, evcls_f_optimizer, train_evcls_f_loader, None, num_epoch, acc_sigmoid, evcls_f_name, full=True)\n",
    "else:\n",
    "    print(f\"{outputs_path}{evcls_f_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/evcls_hard_f.dat does not exist.\n",
      "Using merged dataset ...\n",
      "Epoch 0 ...\n",
      "Iteration 0 of epoch 0 complete. Loss: 0.7082314491271973; Accuracy: 0.46875; Time: 0.23s\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.44683903455734253; Accuracy: 0.8125; Time: 21.01s\n",
      "Iteration 200 of epoch 0 complete. Loss: 0.29592233896255493; Accuracy: 0.875; Time: 21.03s\n",
      "Model is saved to outputs/evcls_hard_f.dat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train: hard negative sampling (merged)\n",
    "evcls_hard_f_name = \"evcls_hard_f\"\n",
    "if not check_file(f\"{outputs_path}{evcls_hard_f_name}.dat\"):\n",
    "    evcls_hard_f_model = EvidenceClassifier()\n",
    "    evcls_hard_f_model.to(device)\n",
    "    evcls_hard_f_criterion = nn.BCEWithLogitsLoss()\n",
    "    evcls_hard_f_optimizer = optim.Adam(evcls_hard_f_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 1\n",
    "    train(evcls_hard_f_model, evcls_hard_f_criterion, evcls_hard_f_optimizer, train_evcls_hard_f_loader, None, num_epoch, acc_sigmoid, evcls_hard_f_name, full=True)\n",
    "else:\n",
    "    print(f\"{outputs_path}{evcls_hard_f_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/evcls.dat\n"
     ]
    }
   ],
   "source": [
    "# Load: random sampling\n",
    "evcls_path = f\"{outputs_path}{evcls_name}.dat\"\n",
    "evcls_model = EvidenceClassifier()\n",
    "evcls_model.load_state_dict(torch.load(evcls_path))\n",
    "evcls_model.to(device)\n",
    "evcls_model.eval()\n",
    "print(f\"Loaded {evcls_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/evcls_hard.dat\n"
     ]
    }
   ],
   "source": [
    "# Load: hard negative sampling\n",
    "evcls_hard_path = f\"{outputs_path}{evcls_hard_name}.dat\"\n",
    "evcls_hard_model = EvidenceClassifier()\n",
    "evcls_hard_model.load_state_dict(torch.load(evcls_hard_path))\n",
    "evcls_hard_model.to(device)\n",
    "evcls_hard_model.eval()\n",
    "print(f\"Loaded {evcls_hard_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/evcls_f.dat\n"
     ]
    }
   ],
   "source": [
    "# Load: random sampling (merged)\n",
    "evcls_f_path = f\"{outputs_path}{evcls_f_name}.dat\"\n",
    "evcls_f_model = EvidenceClassifier()\n",
    "evcls_f_model.load_state_dict(torch.load(evcls_f_path))\n",
    "evcls_f_model.to(device)\n",
    "evcls_f_model.eval()\n",
    "print(f\"Loaded {evcls_f_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/evcls_hard_f.dat\n"
     ]
    }
   ],
   "source": [
    "# Load: hard negative sampling (merged)\n",
    "evcls_hard_f_path = f\"{outputs_path}{evcls_hard_f_name}.dat\"\n",
    "evcls_hard_f_model = EvidenceClassifier()\n",
    "evcls_hard_f_model.load_state_dict(torch.load(evcls_hard_f_path))\n",
    "evcls_hard_f_model.to(device)\n",
    "evcls_hard_f_model.eval()\n",
    "print(f\"Loaded {evcls_hard_f_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evidence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify if the evidence is relevant to the claim\n",
    "def evcls(claim, evidence, model, tokenizer):\n",
    "    \"\"\"Classifies if the provided evidence supports the given claim.\n",
    "\n",
    "    Args:\n",
    "    - claim (str): The claim statement.\n",
    "    - evidence (str): The evidence statement.\n",
    "    - model (torch.nn.Module): The trained model for claim verification.\n",
    "    - tokenizer: Tokenizer for the model.\n",
    "\n",
    "    Returns:\n",
    "    - float: A sigmoid score representing how much the evidence supports the claim.\n",
    "    \"\"\"\n",
    "    # Convert the concatenated claim and evidence into model-ready format\n",
    "    seq, attn_mask, seg_id = convert_input(claim, evidence, tokenizer, token_len)\n",
    "    \n",
    "    # Transfer inputs to GPU\n",
    "    seq = seq.unsqueeze(0).cuda(gpu)\n",
    "    attn_mask = attn_mask.unsqueeze(0).cuda(gpu)\n",
    "    seg_id = seg_id.unsqueeze(0).cuda(gpu)\n",
    "    \n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        return torch.sigmoid(model(seq, attn_mask, seg_id))[0][0]\n",
    "\n",
    "\n",
    "# Retrieve evidences for a claim\n",
    "def ev_retrieve(claim, model, hard_model, n=100, top=5):\n",
    "    \"\"\"Retrieves the top pieces of evidence for a given claim.\n",
    "\n",
    "    Args:\n",
    "    - claim (str): The claim statement.\n",
    "    - model: Model for random sampling.\n",
    "    - hard_model: Model for hard negative sampling.\n",
    "    - n (int, optional): Number of candidates to retrieve using TF-IDF. Defaults to 100.\n",
    "    - top (int, optional): Number of top evidences to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of tuples containing evidence ID and their relevance scores.\n",
    "    \"\"\"\n",
    "    # Get n most similar evidences using TF-IDF\n",
    "    candidates = tfidf_filter(claim, n)\n",
    "    \n",
    "    # Get embedding of the claim for cosine similarity calculation\n",
    "    claim_emd = get_text_embedding([claim], distilbert_model, distilbert_tokenizer, show=False)[0]\n",
    "    \n",
    "    # Clean the claim text (NLTK) for Jaccard similarity calculation\n",
    "    claim_clean = text_clean(claim)\n",
    "    \n",
    "    res = []\n",
    "    for candidate in candidates:\n",
    "        ev_id = candidate[0]\n",
    "        ev = evidence_src[ev_id]\n",
    "        ev_clean = clean_evidence_src[ev_id]\n",
    "        \n",
    "        # Calculate various similarity and relevance scores\n",
    "        tfidf = candidate[1]\n",
    "        jaccard = jaccard_similarity(claim_clean, ev_clean)\n",
    "        distil_sim = cosine_similarity([claim_emd], [ev_embeddings[ev_id]]).item()\n",
    "        \n",
    "        # Get predictions from the models\n",
    "        pred = evcls(claim, ev, model=model, tokenizer=bert_tokenizer).item()\n",
    "        hard_pred = evcls(claim, ev, model=hard_model, tokenizer=bert_tokenizer).item()\n",
    "        \n",
    "        # Calculate a composite score based on the individual scores\n",
    "        score = 0.05 * tfidf + 0.05 * jaccard + 0.2 * distil_sim + 0.4 * pred + 0.3 * hard_pred\n",
    "        res.append((ev_id, score))\n",
    "    \n",
    "    # Sort the results based on the composite score and return the top results\n",
    "    res = sorted(res, key = lambda x: x[1], reverse=True)\n",
    "    return res[:top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the performance of evidence retrieval on a claim in dev set\n",
    "# show; True for printing stats\n",
    "# full: True for using models trained on merged dataset\n",
    "def eval_dev_evcls(index, n=100, top=5, show=True, full=False):\n",
    "    # Select model\n",
    "    model = evcls_f_model if full else evcls_model\n",
    "    hard_model = evcls_hard_f_model if full else evcls_hard_model\n",
    "    \n",
    "    claim = dev_claims[index]\n",
    "    truths = dev_evidences[index]\n",
    "    evidences = [item[0] for item in ev_retrieve(claim, model, hard_model, n, top)]\n",
    "    print(evidences) if show else None\n",
    "    t = 0\n",
    "    f = 0\n",
    "    for truth in truths:\n",
    "        if int(truth[9:]) in evidences:\n",
    "            t += 1\n",
    "            print(f\"In: {truth}\") if show else None\n",
    "        else:\n",
    "            f += 1\n",
    "            print(f\"Out: {truth}\") if show else None\n",
    "    print(f\"In: {t}, Out: {f}\") if show else None\n",
    "    return t, f\n",
    "\n",
    "# Retrieve evidences for all claims from a dataset\n",
    "# check: True for checking number of correct retrieved evidences\n",
    "# full: True for using models trained on merged dataset\n",
    "def ev_retrieve_src(datasrc, n=100, top=2, check=False, full=False):\n",
    "    # Select model\n",
    "    model = evcls_f_model if full else evcls_model\n",
    "    hard_model = evcls_hard_f_model if full else evcls_hard_model\n",
    "\n",
    "    evidences = []\n",
    "    t = 0\n",
    "    f = 0\n",
    "    pbar = tqdm(total=len(datasrc), desc=\"Retrieving evidences\", dynamic_ncols=True)\n",
    "    for i in range(len(datasrc)):\n",
    "        claim = datasrc[i]\n",
    "        evidences.append([item[0] for item in ev_retrieve(claim, model, hard_model, n, top)])\n",
    "        if check:\n",
    "            truths = dev_evidences[i]\n",
    "            for truth in truths:\n",
    "                if int(truth[9:]) in evidences[-1]:\n",
    "                    t += 1\n",
    "                else:\n",
    "                    f += 1\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"Total: {t + f}, In: {t}, Out: {f}\") if check else None\n",
    "    return evidences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section II: Claim Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
    "id2label = {0: 'SUPPORTS', 1: 'REFUTES', 2: 'NOT_ENOUGH_INFO', 3: 'DISPUTED'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT for claim classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for claim classification\n",
    "def create_claimcls_data(claims, evs, labels):\n",
    "    claim_list = []\n",
    "    ev_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i, claim in enumerate(claims):\n",
    "        ev_ids = evs[i]\n",
    "        for num in ev_ids:\n",
    "            ev_id = int(num[9:])\n",
    "            evidence_text = evidence_src[ev_id]\n",
    "            claim_list.append(claim)\n",
    "            ev_list.append(evidence_text)\n",
    "            label_list.append(labels[i])\n",
    "\n",
    "    return pd.DataFrame({'Claim': claim_list, 'Evidence': ev_list, 'Label': label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and development datasets for claim classification\n",
    "train_claimcls = create_claimcls_data(train_claims, train_evidences, train_labels)\n",
    "dev_claimcls = create_claimcls_data(dev_claims, dev_evidences, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training and development datasets for claim classification\n",
    "train_claimcls_f = pd.merge(train_claimcls, dev_claimcls, on=['Claim', 'Evidence', 'Label'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for claim classification\n",
    "class ClaimCLSDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen):\n",
    "        \"\"\"\n",
    "        Constructor for the ClaimCLSDataset\n",
    "        \n",
    "        Args:\n",
    "        - data (pd.DataFrame): DataFrame containing the data. \n",
    "                               Columns should include 'Claim', 'Evidence' and 'Label'\n",
    "        - maxlen (int): Maximum length of the tokenized input (Claim + Evidence). \n",
    "                        Helps in padding/truncating.\n",
    "        \n",
    "        Attributes:\n",
    "        - df (pd.DataFrame): Dataframe containing the data\n",
    "        - tokenizer (Tokenizer): BERT tokenizer\n",
    "        - maxlen (int): Maximum length of the tokenized input.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = data\n",
    "        self.tokenizer = bert_tokenizer  # Assuming bert_tokenizer is pre-defined\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "        - int: Number of samples in the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Fetches the item at a given index from the dataframe and returns tokenized Claim, \n",
    "        Evidence, and their corresponding Label in tensor format.\n",
    "        \n",
    "        Args:\n",
    "        - index (int): Index of the sample to fetch\n",
    "        \n",
    "        Returns:\n",
    "        - tokens_ids_t (tensor): Tokenized and padded ids of the claim and evidence\n",
    "        - attn_mask_t (tensor): Attention mask indicating real tokens (1) vs padded tokens (0)\n",
    "        - seg_ids_t (tensor): Segment ids to distinguish between claim and evidence\n",
    "        - label (tensor): Label of the given sample as tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        claim = self.df.loc[index, 'Claim']\n",
    "        evidence = self.df.loc[index, 'Evidence']\n",
    "        # Convert the label to its corresponding id based on a pre-defined dictionary\n",
    "        label = torch.tensor(label2id[self.df.loc[index, 'Label']])\n",
    "        \n",
    "        # Convert the claim and evidence to the required format for BERT\n",
    "        tokens_ids_t, attn_mask_t, seg_ids_t = convert_input(claim, evidence, self.tokenizer, self.maxlen)\n",
    "        \n",
    "        return tokens_ids_t, attn_mask_t, seg_ids_t, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and development dataloaders for claim classification\n",
    "train_claimcls_set = ClaimCLSDataset(train_claimcls, token_len)\n",
    "dev_claimcls_set = ClaimCLSDataset(dev_claimcls, token_len)\n",
    "train_claimcls_loader = DataLoader(train_claimcls_set, batch_size = 32, shuffle=True)\n",
    "dev_claimcls_loader = DataLoader(dev_claimcls_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader for merged dataset\n",
    "train_claimcls_f_set = ClaimCLSDataset(train_claimcls_f, token_len)\n",
    "train_claimcls_f_loader = DataLoader(train_claimcls_f_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sturcture of the model for Claim classification\n",
    "class ClaimClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClaimClassifier, self).__init__()\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.cls_layer = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg_ids):\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, token_type_ids = seg_ids, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy for model of claim classification\n",
    "def acc_softmax(logits, labels):\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    _, pred_labels = torch.max(probs, dim=1)\n",
    "    total = labels.size(0)\n",
    "    correct = torch.sum(pred_labels == labels).item()\n",
    "    acc = correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/claimcls.dat does not exist.\n",
      "Epoch 0 ...\n",
      "Iteration 0 of epoch 0 complete. Loss: 1.5290237665176392; Accuracy: 0.1875; Time: 0.25s\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.9139746427536011; Accuracy: 0.59375; Time: 22.38s\n",
      "\n",
      "Epoch 0 completed. Development Accuracy: 0.5323153409090909; Development Loss: 1.1569896638393402\n",
      "\n",
      "Best accuracy is improved from 0 to 0.5323153409090909\n",
      "Model is saved to outputs/claimcls.dat\n",
      "\n",
      "Epoch 1 ...\n",
      "Iteration 0 of epoch 1 complete. Loss: 0.7241110801696777; Accuracy: 0.75; Time: 9.33s\n",
      "Iteration 100 of epoch 1 complete. Loss: 0.4082818627357483; Accuracy: 0.84375; Time: 23.8s\n",
      "\n",
      "Epoch 1 completed. Development Accuracy: 0.5220170454545454; Development Loss: 1.50575041025877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "claimcls_name = \"claimcls\"\n",
    "if not check_file(f\"{outputs_path}{claimcls_name}.dat\"):\n",
    "    claimcls_model = ClaimClassifier()\n",
    "    claimcls_model.to(device)\n",
    "    claimcls_criterion = nn.CrossEntropyLoss()\n",
    "    claimcls_optimizer = optim.Adam(claimcls_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 2\n",
    "    train(claimcls_model, claimcls_criterion, claimcls_optimizer, train_claimcls_loader, dev_claimcls_loader, num_epoch, acc_softmax, claimcls_name, cls_type=1)\n",
    "else:\n",
    "    print(f\"{outputs_path}{claimcls_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/claimcls_f.dat does not exist.\n",
      "Using merged dataset ...\n",
      "Epoch 0 ...\n",
      "Iteration 0 of epoch 0 complete. Loss: 1.494035243988037; Accuracy: 0.15625; Time: 0.26s\n",
      "Iteration 100 of epoch 0 complete. Loss: 0.8271755576133728; Accuracy: 0.71875; Time: 22.9s\n",
      "Model is saved to outputs/claimcls_f.dat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on merged dataset\n",
    "claimcls_f_name = \"claimcls_f\"\n",
    "if not check_file(f\"{outputs_path}{claimcls_f_name}.dat\"):\n",
    "    claimcls_f_model = ClaimClassifier()\n",
    "    claimcls_f_model.to(device)\n",
    "    claimcls_f_criterion = nn.CrossEntropyLoss()\n",
    "    claimcls_f_optimizer = optim.Adam(claimcls_f_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 1\n",
    "    train(claimcls_f_model, claimcls_f_criterion, claimcls_f_optimizer, train_claimcls_f_loader, None, num_epoch, acc_softmax, claimcls_f_name, cls_type=1, full=True)\n",
    "else:\n",
    "    print(f\"{outputs_path}{claimcls_f_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/claimcls.dat\n"
     ]
    }
   ],
   "source": [
    "# Load model for claim classification\n",
    "claimcls_path = f\"{outputs_path}{claimcls_name}.dat\"\n",
    "claimcls_model = ClaimClassifier()\n",
    "claimcls_model.load_state_dict(torch.load(claimcls_path))\n",
    "claimcls_model.to(device)\n",
    "claimcls_model.eval()\n",
    "print(f\"Loaded {claimcls_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/claimcls_f.dat\n"
     ]
    }
   ],
   "source": [
    "# Load model for claim classification (merged dataset)\n",
    "claimcls_f_path = f\"{outputs_path}{claimcls_f_name}.dat\"\n",
    "claimcls_f_model = ClaimClassifier()\n",
    "claimcls_f_model.load_state_dict(torch.load(claimcls_f_path))\n",
    "claimcls_f_model.to(device)\n",
    "claimcls_f_model.eval()\n",
    "print(f\"Loaded {claimcls_f_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claim classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a claim\n",
    "def claimcls(claim, evidences, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Classify a claim based on provided evidences using a given model and tokenizer.\n",
    "    \n",
    "    Parameters:\n",
    "    - claim (str): The statement/claim to be classified.\n",
    "    - evidences (list): A list of evidence identifiers.\n",
    "    - model (torch.nn.Module): The pre-trained model for classification.\n",
    "    - tokenizer (object): Tokenizer to process the claim and evidences.\n",
    "    \n",
    "    Returns:\n",
    "    int: The classification label of the claim.\n",
    "    \"\"\"\n",
    "\n",
    "    res = []  # To store the predicted labels for each evidence\n",
    "\n",
    "    # Iterate over each evidence\n",
    "    for ev in evidences:\n",
    "        # Convert the claim and evidence to model input format\n",
    "        seq, attn_mask, seg_id = convert_input(claim, evidence_src[ev], tokenizer, token_len)\n",
    "\n",
    "        # Move the inputs to GPU for faster computation\n",
    "        seq = seq.unsqueeze(0).cuda(gpu)\n",
    "        attn_mask = attn_mask.unsqueeze(0).cuda(gpu)\n",
    "        seg_id = seg_id.unsqueeze(0).cuda(gpu)\n",
    "\n",
    "        # Use the model to make a prediction\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            logits = model(seq, attn_mask, seg_id)  # Get the model's raw outputs (logits)\n",
    "            _, pred = torch.max(torch.softmax(logits, dim=1), dim=1)  # Convert logits to probabilities and get the label with highest probability\n",
    "            pred = pred.item()  # Convert tensor to integer\n",
    "\n",
    "            # If the prediction is not NOT_ENOUGH_INFO\n",
    "            if pred != 2:\n",
    "                res.append(pred)\n",
    "\n",
    "    # If none of 'SUPPORTS', 'REFUTES', 'DISPUTED' are in the list, label it as NOT_ENOUGH_INFO\n",
    "    if len(res) == 0:\n",
    "        return 2\n",
    "\n",
    "    # Return the most common prediction among all evidences\n",
    "    return Counter(res).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on a claim in dev set\n",
    "# show: True for printing predicton and truth\n",
    "def eval_dev_claimcls(index, ev_retrieve, model, tokenizer, show=True):\n",
    "    \"\"\"\n",
    "    Evaluate a claim's classification against the actual label.\n",
    "\n",
    "    Parameters:\n",
    "    - index (int): Index of the claim in the development dataset.\n",
    "    - ev_retrieve (list): List of retrieved evidences.\n",
    "    - model: Pre-trained or fine-tuned model for claim classification.\n",
    "    - tokenizer: Tokenizer corresponding to the model for text processing.\n",
    "    - show (bool, optional): If True, prints prediction and truth. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - int: Model's prediction for the given claim.\n",
    "    \"\"\"\n",
    "    pred = claimcls(dev_claims[index], ev_retrieve[index], model, tokenizer)\n",
    "    truth = label2id[dev_labels[index]]\n",
    "    print(f\"Pred: {pred}, Truth: {truth}\") if show else None\n",
    "    return pred\n",
    "\n",
    "def claimcls_src(claims, evs, model, tokenizer, check=False):\n",
    "    \"\"\"\n",
    "    Classify all claims in the dataset based on the retrieved evidences.\n",
    "\n",
    "    Parameters:\n",
    "    - claims (list): List of claims.\n",
    "    - evs (list): Corresponding list of evidences for each claim.\n",
    "    - model: Pre-trained or fine-tuned model for claim classification.\n",
    "    - tokenizer: Tokenizer corresponding to the model for text processing.\n",
    "    - check (bool, optional): If True, checks accuracy of predictions against truth labels. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of predictions for each claim.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    t = 0\n",
    "    f = 0\n",
    "    # Initializing the progress bar for visual feedback during prediction\n",
    "    pbar = tqdm(total=len(claims), desc=\"Predicting claims\", dynamic_ncols=True)\n",
    "\n",
    "    for i in range(len(claims)):\n",
    "        pred = claimcls(claims[i], evs[i], model, tokenizer)\n",
    "        preds.append(pred)\n",
    "        if check:\n",
    "            # Verifying the prediction against the actual label for accuracy calculation\n",
    "            truth = label2id[dev_labels[i]]\n",
    "            if pred == truth:\n",
    "                t += 1\n",
    "            else:\n",
    "                f += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    # Displaying summary of the prediction results if check is True\n",
    "    print(f\"Total: {t + f}, Correct: {t}, Wrong: {f}, Accuracy: {round(t / (t + f), 2)}\") if check else None\n",
    "    return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section III: Evaluation & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format evidences for saving\n",
    "def format_evidences(evidences):\n",
    "    res = []\n",
    "    for ev in evidences:\n",
    "        res.append(f\"evidence-{ev}\")\n",
    "    return res\n",
    "\n",
    "# Formatting and saving results\n",
    "def save_results(claim_ids, claims, labels, evidences, filename):\n",
    "    data = {}\n",
    "    for i in range(len(claim_ids)):\n",
    "        claim_id = claim_ids[i]\n",
    "        claim_text = claims[i]\n",
    "        claim_label = id2label[labels[i]]\n",
    "        evs = format_evidences(evidences[i])\n",
    "        data[claim_id] = {\"claim_text\": claim_text,\n",
    "                          \"claim_label\": claim_label,\n",
    "                          \"evidences\": evs}\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711c30911bf74d2184bee21e7b99fd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving evidences:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 491, In: 115, Out: 376\n"
     ]
    }
   ],
   "source": [
    "# Retrieve evidences for claims in dev set\n",
    "dev_ev_retrieve = ev_retrieve_src(dev_claims, 150, 5, check=True, full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdbddb778214682b9096837eb37d6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting claims:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 154, Correct: 77, Wrong: 77, Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Classify claims in dev set\n",
    "dev_claimcls = claimcls_src(dev_claims, dev_ev_retrieve, claimcls_model, bert_tokenizer, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to prediction/dev-pred.json\n"
     ]
    }
   ],
   "source": [
    "dev_pred_output = \"dev-pred.json\"\n",
    "dev_save_path = f\"{prediction_path}{dev_pred_output}\"\n",
    "save_results(dev_claim_ids, dev_claims, dev_claimcls, dev_ev_retrieve, dev_save_path)\n",
    "print(f\"Saved to {dev_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence Retrieval F-score (F)    = 0.17878787878787883\n",
      "Claim Classification Accuracy (A) = 0.5\n",
      "Harmonic Mean of F and A          = 0.2633928571428572\n"
     ]
    }
   ],
   "source": [
    "if check_file(\"eval.py\"):\n",
    "    !python eval.py --predictions data/dev-claims.json --groundtruth prediction/dev-pred.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = f\"{data_dir}test-claims-unlabelled.json\"\n",
    "test_claim_ids, test_claims = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9457b1339945378a75575431b6c920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving evidences:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve evidences for claims in test set\n",
    "# test_ev_retrieve = ev_retrieve_src(test_claims, 150, 5, full=False)\n",
    "test_ev_retrieve = ev_retrieve_src(test_claims, 150, 5, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2aec18dd9434c08b1489bde72bcee7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting claims:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classify claims in test set\n",
    "# test_claimcls = claimcls_src(test_claims, test_ev_retrieve, claimcls_model, bert_tokenizer)\n",
    "test_claimcls = claimcls_src(test_claims, test_ev_retrieve, claimcls_f_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to prediction/test-claims-predictions.json\n"
     ]
    }
   ],
   "source": [
    "test_pred_output = \"test-claims-predictions.json\"\n",
    "test_save_path = f\"{prediction_path}{test_pred_output}\"\n",
    "save_results(test_claim_ids, test_claims, test_claimcls, test_ev_retrieve, test_save_path)\n",
    "print(f\"Saved to {test_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_insight(index):\n",
    "    print(f\"Claim: {test_claims[index]}\")\n",
    "    for ev in test_ev_retrieve[index]:\n",
    "        print(evidence_src[ev])\n",
    "    print(f\"Prediction: {id2label[test_claimcls[index]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: The contribution of waste heat to the global climate is 0.028 W/m2.\n",
      "Global forcing from waste heat was 0.028 W/m2 in 2005.\n",
      "The global temperature increase since the beginning of the industrial period (taken as 1750) is about 0.8C (1.4F), and the radiative forcing due to CO 2 and other long-lived greenhouse gases mainly methane, nitrous oxide, and chlorofluorocarbons emitted since that time is about 2.6 W/m2.\n",
      "Taking planetary heat uptake rate as the rate of ocean heat uptake estimated by the IPCC AR4 as 0.2 W/m2, yields a value for S of 2.1C (3.8F).\n",
      "Without feedbacks the radiative forcing of approximately 3.7 W/m2, due to doubling CO 2 from the pre-industrial 280 ppm, would eventually result in roughly 1C global warming.\n",
      "Solar irradiance is about 0.9 W/m2 brighter during solar maximum than during solar minimum, which correlated in measured average global temperature over the period 1959-2004.\n",
      "Prediction: SUPPORTS\n"
     ]
    }
   ],
   "source": [
    "test_insight(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
